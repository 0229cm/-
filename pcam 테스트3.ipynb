{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding ='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding ='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1, 1), padding = 'same')\n",
    "        \n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "\n",
    "# %%\n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        for unit in self.sequence:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "    \n",
    "# %%\n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(8, (3, 3), padding = 'same', activation='relu') # 28x28x8\n",
    "        \n",
    "        self.res1 = ResnetLayer(8, (16, 16), (3, 3)) # 28x28x16\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
    "        \n",
    "        self.res2 = ResnetLayer(16, (32, 32), (3, 3)) # 14x14x32\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        self.res3 = ResnetLayer(32, (64, 64), (3, 3)) # 7x7x64\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.res1(x, training = training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training = training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training = training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "# %%\n",
    "# Implement training loop\n",
    "@tf.function\n",
    "def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "# Implement algorithm test\n",
    "@tf.function\n",
    "def test_step(model, images, labels, loss_object, test_loss, test_accuracy):\n",
    "    predictions = model(images, training=False)\n",
    "\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "# %% \n",
    "def get_Pcam_ds():\n",
    "    (train_validation_ds, test_ds), ds_info = tfds.load(name='patch_camelyon',\n",
    "                                                            shuffle_files=True,\n",
    "                                                            as_supervised=True,\n",
    "                                                            split=['train', 'test'],\n",
    "                                                            with_info=True)\n",
    "    \n",
    "    n_train_validation = ds_info.splits['train'].num_examples\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    n_train = int(n_train_validation * train_ratio)\n",
    "    n_validation = n_train_validation - n_train\n",
    "    \n",
    "    train_ds = train_validation_ds.take(n_train)\n",
    "    remaining_ds = train_validation_ds.skip(n_train)\n",
    "    validation_ds = remaining_ds.take(n_validation)\n",
    "    \n",
    "    return train_ds, validation_ds, test_ds\n",
    "\n",
    "# %%\n",
    "def standardization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
    "    global train_ds, validation_ds, test_ds\n",
    "    \n",
    "    def stnd(images, labels):\n",
    "        images = tf.cast(images, tf.float32) / 255.\n",
    "        return (images, labels)\n",
    "    \n",
    "    train_ds = train_ds.map(stnd).shuffle(1000).batch(TRAIN_BATCH_SIZE)\n",
    "    validation_ds = validation_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
    "    test_ds = test_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
    "    \n",
    "# %%\n",
    "\n",
    "class Pcam_Classifier(Model):\n",
    "    def __init__(self):\n",
    "        super(Pcam_Classifier, self).__init__()\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(64, activation='relu')\n",
    "        self.d2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return x\n",
    "# %% \n",
    "def load_metrics():\n",
    "    global train_loss, train_acc\n",
    "    global validation_loss, validation_acc\n",
    "    global test_loss, test_acc\n",
    "    \n",
    "    train_loss = Mean()\n",
    "    validation_loss = Mean()\n",
    "    test_loss = Mean()\n",
    "    \n",
    "    train_acc = SparseCategoricalAccuracy()\n",
    "    validation_acc = SparseCategoricalAccuracy()\n",
    "    test_acc = SparseCategoricalAccuracy()\n",
    "\n",
    "# %%\n",
    "def trainer():\n",
    "    global train_ds, model, loss_object, optimizer\n",
    "    global train_loss, train_acc\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images)\n",
    "            loss = loss_object(labels, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_acc(labels, predictions)\n",
    "        \n",
    "# %%\n",
    "def validation():\n",
    "    global validation_ds, model, loss_object\n",
    "    global validation_loss, validation_acc\n",
    "\n",
    "    for images, labels in validation_ds:\n",
    "        \n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        \n",
    "        validation_loss(loss)\n",
    "        validation_acc(labels, predictions)\n",
    "\n",
    "# %%\n",
    "def tester():\n",
    "    global test_ds, model, loss_object\n",
    "    global test_loss, test_acc\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        \n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        \n",
    "        test_loss(loss)\n",
    "        test_acc(labels, predictions)\n",
    "\n",
    "    template = 'Test Loss: {:.4f}\\t Test Accuracy: {:.2f}%\\n'\n",
    "    print(template.format(test_loss.result(),\n",
    "                          test_acc.result()*100))\n",
    " # %%   \n",
    "def train_reporter():\n",
    "    global epoch\n",
    "    global train_loss, train_acc\n",
    "    global validation_loss, validation_acc\n",
    "\n",
    "    print(colored('Epoch', 'red', 'on_white'), epoch + 1)\n",
    "    template = 'Train Loss: {:.4f}\\t Train Accuracy: {:.2f}%\\n' + 'Validation Loss: {:.4f}\\t Validation Accuracy: {:.2f}%\\n'\n",
    "        \n",
    "    print(template.format(train_loss.result(),\n",
    "                          train_acc.result()*100,\n",
    "                          validation_loss.result(),\n",
    "                          validation_acc.result()*100))\n",
    "    \n",
    "    train_acc.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    validation_acc.reset_states() \n",
    "# %%\n",
    "\n",
    "# Create model\n",
    "model = ResNet()\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define performance metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[47m\u001b[31mEpoch\u001b[0m 1\n",
      "Train Loss: 0.6208\t Train Accuracy: 64.38%\n",
      "Validation Loss: 0.9001\t Validation Accuracy: 50.39%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 2\n",
      "Train Loss: 0.5879\t Train Accuracy: 67.09%\n",
      "Validation Loss: 0.6550\t Validation Accuracy: 60.58%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 3\n",
      "Train Loss: 0.5770\t Train Accuracy: 68.32%\n",
      "Validation Loss: 0.5572\t Validation Accuracy: 69.42%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 4\n",
      "Train Loss: 0.5682\t Train Accuracy: 69.46%\n",
      "Validation Loss: 0.6098\t Validation Accuracy: 65.60%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 5\n",
      "Train Loss: 0.5608\t Train Accuracy: 70.54%\n",
      "Validation Loss: 0.5816\t Validation Accuracy: 68.00%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 6\n",
      "Train Loss: 0.5526\t Train Accuracy: 71.60%\n",
      "Validation Loss: 0.5429\t Validation Accuracy: 73.88%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 7\n",
      "Train Loss: 0.5453\t Train Accuracy: 72.42%\n",
      "Validation Loss: 0.5564\t Validation Accuracy: 70.77%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 8\n",
      "Train Loss: 0.5384\t Train Accuracy: 73.17%\n",
      "Validation Loss: 0.5914\t Validation Accuracy: 67.46%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 9\n",
      "Train Loss: 0.5331\t Train Accuracy: 73.69%\n",
      "Validation Loss: 0.5706\t Validation Accuracy: 69.47%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "train_ds, validation_ds, test_ds = get_Pcam_ds()\n",
    "standardization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)\n",
    "\n",
    "model = Pcam_Classifier()\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "optimizer = SGD(learning_rate=LR)\n",
    "\n",
    "load_metrics()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    trainer()\n",
    "    validation()\n",
    "    train_reporter()\n",
    "\n",
    "tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
