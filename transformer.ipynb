{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda, Layer, Embedding, LayerNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\min'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "NUM_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot-Scaled Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotScaledAttention(Layer):\n",
    "    def __init__(self, d_emb, d_reduced, masked=False):\n",
    "        super().__init__()\n",
    "        self.q = Dense(d_reduced, input_shape=(-1, d_emb))\n",
    "        self.k = Dense(d_reduced, input_shape=(-1, d_emb))\n",
    "        self.v = Dense(d_reduced, input_shape=(-1, d_emb))\n",
    "        self.scale = Lambda(lambda x: x/np.sqrt(d_reduced))\n",
    "        self.masked = masked\n",
    "\n",
    "    def call(self, x, training=None, mask=None): # (q,k,v)\n",
    "        q = self.scale(self.q(x[0]))\n",
    "        k = self.k(x[1])\n",
    "        v = self.v(x[2])\n",
    "        \n",
    "        k_T = tf.transpose(k, perm=[0, 2, 1])\n",
    "        comp = tf.matmul(q, k_T)\n",
    "        \n",
    "        if self.masked: # Referred from https://github.com/LastRemote/Transformer-TF2.0\n",
    "            length = tf.shape(comp)[-1]\n",
    "            mask = tf.fill((length, length), -np.inf)\n",
    "            mask = tf.linalg.band_part(mask, 0, -1) # Get upper triangle\n",
    "            mask = tf.linalg.set_diag(mask, tf.zeros((length))) # Set diagonal to zeros to avoid operations with infinity\n",
    "            comp += mask\n",
    "        comp = tf.nn.softmax(comp, axis=-1)\n",
    "        return tf.matmul(comp, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, num_head, d_emb, d_reduced, masked=False):\n",
    "        super().__init__()\n",
    "        self.attention_list = list()\n",
    "        for _ in range(num_head):\n",
    "            self.attention_list.append(DotScaledAttention(d_emb, d_reduced, masked))\n",
    "        self.linear = Dense(d_emb, input_shape=(-1, num_head * d_reduced))\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        attention_list = [a(x) for a in self.attention_list]\n",
    "        concat = tf.concat(attention_list, axis=-1)\n",
    "        return self.linear(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, num_head, d_reduced):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.d_r = d_reduced\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.multi_attention = MultiHeadAttention(self.num_head, input_shape[-1], self.d_r)\n",
    "        self.layer_norm1 = LayerNormalization(input_shape=input_shape)\n",
    "        self.dense1 = Dense(input_shape[-1] * 4, input_shape=input_shape, activation='relu')\n",
    "        self.dense2 = Dense(input_shape[-1],\n",
    "                            input_shape=self.dense1.compute_output_shape(input_shape))\n",
    "        self.layer_norm2 = LayerNormalization(input_shape=input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        h = self.multi_attention((x, x, x))\n",
    "        ln1 = self.layer_norm1(x + h)\n",
    "        \n",
    "        h = self.dense2(self.dense1(ln1))\n",
    "        return self.layer_norm2(h + ln1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(self, num_head, d_reduced):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.d_r = d_reduced\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.self_attention = MultiHeadAttention(self.num_head, input_shape[0][-1], self.d_r, masked=True)\n",
    "        self.layer_norm1 = LayerNormalization(input_shape=input_shape)\n",
    "        \n",
    "        self.multi_attention = MultiHeadAttention(self.num_head, input_shape[0][-1], self.d_r)\n",
    "        self.layer_norm2 = LayerNormalization(input_shape=input_shape)\n",
    "        \n",
    "        self.dense1 = Dense(input_shape[0][-1] * 4, input_shape=input_shape[0], activation='relu')\n",
    "        self.dense2 = Dense(input_shape[0][-1],\n",
    "                            input_shape=self.dense1.compute_output_shape(input_shape[0]))\n",
    "        self.layer_norm3 = LayerNormalization(input_shape=input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None): # (x, context)\n",
    "        x, context = inputs\n",
    "        h = self.self_attention((x, x, x))\n",
    "        ln1 = self.layer_norm1(x + h)\n",
    "        \n",
    "        h = self.multi_attention((ln1, context, context))\n",
    "        ln2 = self.layer_norm2(ln1 + h)\n",
    "        \n",
    "        h = self.dense2(self.dense1(ln2))\n",
    "        return self.layer_norm3(h + ln2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Layer):  # Referred from https://github.com/LastRemote/Transformer-TF2.0\n",
    "    def __init__(self, max_len, d_emb):\n",
    "        super().__init__()\n",
    "        self.sinusoidal_encoding = np.array([self.get_positional_angle(pos, d_emb) for pos in range(max_len)], dtype=np.float32)\n",
    "        self.sinusoidal_encoding[:, 0::2] = np.sin(self.sinusoidal_encoding[:, 0::2])\n",
    "        self.sinusoidal_encoding[:, 1::2] = np.cos(self.sinusoidal_encoding[:, 1::2])\n",
    "        self.sinusoidal_encoding = tf.cast(self.sinusoidal_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        return x + self.sinusoidal_encoding[:tf.shape(x)[1]]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_angle(self, pos, dim, d_emb):\n",
    "        return pos / np.power(10000, 2 * (dim // 2) / d_emb)\n",
    "\n",
    "    def get_positional_angle(self, pos, d_emb):\n",
    "        return [self.get_angle(pos, dim, d_emb) for dim in range(d_emb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(self, src_vocab, dst_vocab, max_len, d_emb,\n",
    "                       d_reduced, n_enc_layer, n_dec_layer, num_head):\n",
    "        super().__init__()\n",
    "        self.enc_emb = Embedding(src_vocab, d_emb)\n",
    "        self.dec_emb = Embedding(dst_vocab, d_emb)\n",
    "        self.pos_enc = PositionalEncoding(max_len, d_emb)\n",
    "        \n",
    "        self.encoder = [Encoder(num_head, d_reduced) for _ in range(n_enc_layer)]\n",
    "        self.decoder = [Decoder(num_head, d_reduced) for _ in range(n_dec_layer)]\n",
    "        self.dense = Dense(dst_vocab, input_shape=(-1, d_emb))\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None): # (src_sentence, dst_sentence_shifted)\n",
    "        src_sent, dst_sent_shifted = inputs\n",
    "        \n",
    "        h_enc = self.pos_enc(self.enc_emb(src_sent))\n",
    "        for enc in self.encoder:\n",
    "            h_enc = enc(h_enc)\n",
    "            \n",
    "        h_dec = self.pos_enc(self.dec_emb(dst_sent_shifted))\n",
    "        for dec in self.decoder:\n",
    "            h_dec = dec([h_dec, h_enc])\n",
    "            \n",
    "        return tf.nn.softmax(self.dense(h_dec), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'UrbanSound8K1.txt' # acquired from 'http://www.aihub.or.kr' and modified\n",
    "okt = Okt()\n",
    "\n",
    "with open(dataset_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    seq = [' '.join(okt.morphs(line)) for line in lines]\n",
    "\n",
    "questions = seq[::2]\n",
    "answers = ['\\t ' + lines for lines in seq[1::2]]\n",
    "\n",
    "num_sample = len(questions)\n",
    "\n",
    "perm = list(range(num_sample))\n",
    "random.seed(0)\n",
    "random.shuffle(perm)\n",
    "\n",
    "train_q = list()\n",
    "train_a = list()\n",
    "test_q = list()\n",
    "test_a = list()\n",
    "\n",
    "for idx, qna in enumerate(zip(questions, answers)):\n",
    "    q, a = qna\n",
    "    if perm[idx] > num_sample//5:\n",
    "        train_q.append(q)\n",
    "        train_a.append(a)\n",
    "    else:\n",
    "        test_q.append(q)\n",
    "        test_a.append(a)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "\n",
    "tokenizer.fit_on_texts(train_q + train_a)\n",
    "\n",
    "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
    "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
    "\n",
    "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
    "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=64)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=65)\n",
    "y_train_shifted = np.concatenate([np.zeros((y_train.shape[0], 1)), y_train[:, 1:]], axis=1)\n",
    "\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=64)\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3492 samples\n",
      "Epoch 1/200\n",
      "3492/3492 [==============================] - 95s 27ms/sample - loss: 1.8698 - accuracy: 0.8164\n",
      "Epoch 2/200\n",
      "3492/3492 [==============================] - 68s 20ms/sample - loss: 0.4049 - accuracy: 0.9419\n",
      "Epoch 3/200\n",
      "3492/3492 [==============================] - 35s 10ms/sample - loss: 0.2886 - accuracy: 0.9534\n",
      "Epoch 4/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 0.2275 - accuracy: 0.9612\n",
      "Epoch 5/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 0.1824 - accuracy: 0.9687\n",
      "Epoch 6/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.1445 - accuracy: 0.9758\n",
      "Epoch 7/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.1117 - accuracy: 0.9818\n",
      "Epoch 8/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0843 - accuracy: 0.9874\n",
      "Epoch 9/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0621 - accuracy: 0.9917\n",
      "Epoch 10/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0447 - accuracy: 0.9945\n",
      "Epoch 11/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0316 - accuracy: 0.9961\n",
      "Epoch 12/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0219 - accuracy: 0.9968\n",
      "Epoch 13/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0154 - accuracy: 0.9981\n",
      "Epoch 14/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0108 - accuracy: 0.9989\n",
      "Epoch 15/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0072 - accuracy: 0.9996\n",
      "Epoch 16/200\n",
      "3492/3492 [==============================] - 39s 11ms/sample - loss: 0.0049 - accuracy: 0.9998\n",
      "Epoch 17/200\n",
      "3492/3492 [==============================] - 41s 12ms/sample - loss: 0.0032 - accuracy: 0.9999\n",
      "Epoch 18/200\n",
      "3492/3492 [==============================] - 37s 11ms/sample - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 6.2915e-04 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 3.4344e-04 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "3492/3492 [==============================] - 41s 12ms/sample - loss: 2.1810e-04 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "3492/3492 [==============================] - 39s 11ms/sample - loss: 1.5307e-04 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "3492/3492 [==============================] - 39s 11ms/sample - loss: 1.0984e-04 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "3492/3492 [==============================] - 40s 11ms/sample - loss: 7.8663e-05 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "3492/3492 [==============================] - 41s 12ms/sample - loss: 5.6593e-05 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "3492/3492 [==============================] - 42s 12ms/sample - loss: 4.0298e-05 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "3492/3492 [==============================] - 41s 12ms/sample - loss: 2.8769e-05 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 2.0803e-05 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 1.5060e-05 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 0.0048 - accuracy: 0.9989\n",
      "Epoch 32/200\n",
      "3492/3492 [==============================] - 37s 11ms/sample - loss: 4.5327e-04 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 2.2873e-04 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 6.1818e-05 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 3.3644e-05 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 2.5542e-05 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 2.1064e-05 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "3492/3492 [==============================] - 39s 11ms/sample - loss: 1.7602e-05 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "3492/3492 [==============================] - 39s 11ms/sample - loss: 1.4704e-05 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 1.2143e-05 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "3492/3492 [==============================] - 38s 11ms/sample - loss: 9.9237e-06 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "3492/3492 [==============================] - 36s 10ms/sample - loss: 8.0221e-06 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "3492/3492 [==============================] - 35s 10ms/sample - loss: 6.3994e-06 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 5.2439e-06 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "3492/3492 [==============================] - 35s 10ms/sample - loss: 3.9447e-06 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.9102e-06 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.1271e-06 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.5550e-06 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.1379e-06 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "3492/3492 [==============================] - 36s 10ms/sample - loss: 8.4553e-07 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 6.3610e-07 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "3492/3492 [==============================] - 35s 10ms/sample - loss: 4.8075e-07 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 3.6905e-07 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.8655e-07 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.2629e-07 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 1.8397e-07 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.5758e-07 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 59/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 60/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 6.3639e-04 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 4.7789e-05 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 8.8004e-06 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 6.1841e-06 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 5.0318e-06 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 4.1864e-06 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 3.5025e-06 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.9395e-06 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.4696e-06 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.0692e-06 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.7373e-06 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.4475e-06 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.1960e-06 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 9.8681e-07 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 8.1335e-07 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 6.6935e-07 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 5.4652e-07 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "3492/3492 [==============================] - 32s 9ms/sample - loss: 4.4554e-07 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.6376e-07 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "3492/3492 [==============================] - 32s 9ms/sample - loss: 2.9985e-07 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "3492/3492 [==============================] - 32s 9ms/sample - loss: 2.5249e-07 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "3492/3492 [==============================] - 32s 9ms/sample - loss: 2.3012e-07 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 83/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 84/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.2906e-04 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 1.5035e-05 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 7.6115e-06 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 5.9444e-06 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 4.8288e-06 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.9761e-06 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.2905e-06 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.7315e-06 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.2676e-06 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.8816e-06 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.5586e-06 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.2883e-06 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.0606e-06 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 8.6871e-07 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 7.1149e-07 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 5.7849e-07 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "3492/3492 [==============================] - ETA: 0s - loss: 4.7328e-07 - accuracy: 1.00 - 33s 10ms/sample - loss: 4.7289e-07 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 3.8701e-07 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.2018e-07 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.7233e-07 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.7424e-07 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 0.0031 - accuracy: 0.9991\n",
      "Epoch 106/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 8.4008e-04 - accuracy: 0.9998\n",
      "Epoch 107/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 3.6385e-05 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 7.7316e-06 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 5.2524e-06 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 4.1954e-06 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 3.4418e-06 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.8476e-06 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.3565e-06 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 1.9555e-06 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 1.6214e-06 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.3414e-06 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.1074e-06 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 9.1037e-07 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 7.4597e-07 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 6.1124e-07 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 5.0350e-07 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 4.1881e-07 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 3.8553e-07 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 4.5063e-07 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 126/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 7.2446e-04 - accuracy: 0.9998\n",
      "Epoch 127/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 3.4520e-04 - accuracy: 0.9999\n",
      "Epoch 128/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 1.9588e-05 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 5.5337e-06 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 3.5585e-06 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.8290e-06 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.3018e-06 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 1.8881e-06 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.5498e-06 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.2753e-06 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "3492/3492 [==============================] - 32s 9ms/sample - loss: 1.0493e-06 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 8.6768e-07 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 7.1427e-07 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 5.8975e-07 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 4.8719e-07 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 4.0275e-07 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 3.3635e-07 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.8356e-07 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.4578e-07 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 2.2064e-07 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 2.5752e-07 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 148/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 9.6601e-04 - accuracy: 0.9998\n",
      "Epoch 149/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.1957e-04 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.0820e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 4.9951e-06 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.6417e-06 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.9106e-06 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.3739e-06 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.9388e-06 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.5925e-06 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.3093e-06 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "3492/3492 [==============================] - 34s 10ms/sample - loss: 1.0767e-06 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 8.8409e-07 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 7.2538e-07 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 5.9873e-07 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 5.0980e-07 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 4.0268e-07 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.3512e-07 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.9133e-07 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.8735e-07 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 9.1589e-04 - accuracy: 0.9997\n",
      "Epoch 168/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 169/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.4441e-04 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 1.5823e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 4.6363e-06 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.5505e-06 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.8676e-06 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.3483e-06 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.9315e-06 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.5870e-06 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.3085e-06 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.0781e-06 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 8.8415e-07 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 7.2096e-07 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 5.8533e-07 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 4.7759e-07 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.9258e-07 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 3.2893e-07 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 2.8350e-07 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.5417e-07 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 2.6536e-07 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 189/200\n",
      "3492/3492 [==============================] - 33s 10ms/sample - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 190/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 8.1648e-05 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "3492/3492 [==============================] - 32s 9ms/sample - loss: 9.8750e-06 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 5.2848e-06 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.9908e-06 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 3.1759e-06 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.5427e-06 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 2.0509e-06 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.6569e-06 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.3391e-06 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 1.0841e-06 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "3492/3492 [==============================] - 33s 9ms/sample - loss: 8.7302e-07 - accuracy: 1.0000\n",
      "Time :  6917.311845779419\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "transformer = Transformer(NUM_WORDS,\n",
    "                          NUM_WORDS,\n",
    "                          128, 16, 16, 2, 2, 4) # Instantiating a new transformer model\n",
    "\n",
    "transformer.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "transformer.fit([x_train, y_train_shifted], y_train, batch_size=5, epochs=EPOCHS)\n",
    "print(\"Time : \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
