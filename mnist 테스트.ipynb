{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_ds():\n",
    "    (train_validation_ds, test_ds), ds_info = tfds.load(name='mnist',\n",
    "                                                            shuffle_files=True,\n",
    "                                                            as_supervised=True,\n",
    "                                                            split=['train', 'test'],\n",
    "                                                            with_info=True)\n",
    "    \n",
    "    n_train_validation = ds_info.splits['train'].num_examples\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    n_train = int(n_train_validation * train_ratio)\n",
    "    n_validation = n_train_validation - n_train\n",
    "    \n",
    "    train_ds = train_validation_ds.take(n_train)\n",
    "    remaining_ds = train_validation_ds.skip(n_train)\n",
    "    validation_ds = remaining_ds.take(n_validation)\n",
    "    \n",
    "    return train_ds, validation_ds, test_ds\n",
    "\n",
    "\n",
    "# standardization , float32로 변경\n",
    "\n",
    "def standardization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
    "    global train_ds, validation_ds, test_ds\n",
    "    \n",
    "    def stnd(images, labels):\n",
    "        images = tf.cast(images, tf.float32) / 255.\n",
    "        return (images, labels)\n",
    "    \n",
    "    train_ds = train_ds.map(stnd).shuffle(1000).batch(TRAIN_BATCH_SIZE)\n",
    "    validation_ds = validation_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
    "    test_ds = test_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
    "\n",
    "# 모델 생성\n",
    "class MNIST_Classifier(Model):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Classifier, self).__init__()\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(64, activation='relu')\n",
    "        self.d2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return x\n",
    "\n",
    "# 코드 간결화\n",
    "def load_metrics():\n",
    "    global train_loss, train_acc\n",
    "    global validation_loss, validation_acc\n",
    "    global test_loss, test_acc\n",
    "    \n",
    "    train_loss = Mean()\n",
    "    validation_loss = Mean()\n",
    "    test_loss = Mean()\n",
    "    \n",
    "    train_acc = SparseCategoricalAccuracy()\n",
    "    validation_acc = SparseCategoricalAccuracy()\n",
    "    test_acc = SparseCategoricalAccuracy()\n",
    "    \n",
    "# training step\n",
    "\n",
    "\n",
    "def trainer():\n",
    "    global train_ds, model, loss_object, optimizer\n",
    "    global train_loss, train_acc\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images)\n",
    "            loss = loss_object(labels, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_acc(labels, predictions)\n",
    "        \n",
    "\n",
    "def validation():\n",
    "    global validation_ds, model, loss_object\n",
    "    global validation_loss, validation_acc\n",
    "\n",
    "    for images, labels in validation_ds:\n",
    "        \n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        \n",
    "        validation_loss(loss)\n",
    "        validation_acc(labels, predictions)\n",
    "\n",
    "\n",
    "def tester():\n",
    "    global test_ds, model, loss_object\n",
    "    global test_loss, test_acc\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        \n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        \n",
    "        test_loss(loss)\n",
    "        test_acc(labels, predictions)\n",
    "\n",
    "# 리포터\n",
    "def train_reporter():\n",
    "    global epoch\n",
    "    global train_loss, train_acc\n",
    "    global validation_loss, validation_acc\n",
    "\n",
    "    print(colored('Epoch', 'red', 'on_white'), epoch +1)\n",
    "    template = 'Train Loss: {:.4f}\\t Train Accuracy: {:.2f}%\\n' +\\\n",
    "        'Validation Loss: {:.4f}\\t Validation Accuracy: {:.2f}%\\n'\n",
    "        \n",
    "    print(template.format(train_loss.result(),\n",
    "                          train_acc.result() * 100,\n",
    "                          validation_loss.result(),\n",
    "                          validation_acc.result() * 100))\n",
    "    \n",
    "    train_acc.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    validation_acc.reset_states() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[47m\u001b[31mEpoch\u001b[0m 1\n",
      "Train Loss: 1.4768\t Train Accuracy: 62.01%\n",
      "Validation Loss: 0.9401\t Validation Accuracy: 78.62%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 2\n",
      "Train Loss: 0.7562\t Train Accuracy: 82.02%\n",
      "Validation Loss: 0.6376\t Validation Accuracy: 84.53%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 3\n",
      "Train Loss: 0.5708\t Train Accuracy: 85.56%\n",
      "Validation Loss: 0.5230\t Validation Accuracy: 86.51%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 4\n",
      "Train Loss: 0.4875\t Train Accuracy: 87.25%\n",
      "Validation Loss: 0.4623\t Validation Accuracy: 87.69%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 5\n",
      "Train Loss: 0.4395\t Train Accuracy: 88.34%\n",
      "Validation Loss: 0.4246\t Validation Accuracy: 88.61%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 6\n",
      "Train Loss: 0.4078\t Train Accuracy: 89.00%\n",
      "Validation Loss: 0.3988\t Validation Accuracy: 89.22%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 7\n",
      "Train Loss: 0.3851\t Train Accuracy: 89.51%\n",
      "Validation Loss: 0.3795\t Validation Accuracy: 89.62%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 8\n",
      "Train Loss: 0.3677\t Train Accuracy: 89.86%\n",
      "Validation Loss: 0.3651\t Validation Accuracy: 89.97%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 9\n",
      "Train Loss: 0.3536\t Train Accuracy: 90.19%\n",
      "Validation Loss: 0.3529\t Validation Accuracy: 90.26%\n",
      "\n",
      "\u001b[47m\u001b[31mEpoch\u001b[0m 10\n",
      "Train Loss: 0.3419\t Train Accuracy: 90.51%\n",
      "Validation Loss: 0.3429\t Validation Accuracy: 90.42%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "train_ds, validation_ds, test_ds = get_mnist_ds()\n",
    "standardization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)\n",
    "\n",
    "model = MNIST_Classifier()\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "optimizer = SGD(learning_rate=LR)\n",
    "\n",
    "load_metrics()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    trainer()\n",
    "    validation()\n",
    "    train_reporter()\n",
    "\n",
    "tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
